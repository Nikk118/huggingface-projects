# -*- coding: utf-8 -*-
"""finetuning_minilm_for_emotion_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gvfx01DG6Bgm9mff6lztd78qGG8Yu4lh

#
Simple Training with the ðŸ¤— Transformers Trainer
"""

from datasets import load_dataset
emotion_dataset=load_dataset("emotion")
emotion_dataset

emotion_dataset["train"][0]

emotion_df=emotion_dataset["train"].to_pandas()
emotion_df.head()

features=emotion_dataset["train"].features
features

id2label={idx:features["label"].int2str(idx) for idx in range(6)}
id2label

label2id={v:k for k,v in id2label.items()}
label2id

emotion_df["label"].value_counts(normalize=True).sort_index()

"""# Tokanize all the things"""

from transformers import AutoTokenizer
model_ckpt="microsoft/MiniLM-L12-H384-uncased"
tokenizer=AutoTokenizer.from_pretrained(model_ckpt)

tokenizer(emotion_dataset["train"]["text"][:1])

def tokenize_text(example):
  return tokenizer(example["text"],truncation=True,max_length=512)

emotion_dataset=emotion_dataset.map(tokenize_text,batched=True)
emotion_dataset

"""#dealing with class imbalance"""

class_weights=(1-(emotion_df["label"].value_counts().sort_index()/len(emotion_df))).values
class_weights

import torch
class_weights=torch.from_numpy(class_weights).float().to("cuda")
class_weights

emotion_dataset=emotion_dataset.rename_columns({"label":"labels"})

from torch import nn
import torch
from transformers import Trainer

class weightsLossTrainer(Trainer):
  def compute_loss(self,model,inputs,return_outputs=False,**kwargs):
    outputs=model(**inputs)
    logits=outputs.get("logits")
    labels=inputs.get("labels")
    loss_fn=nn.CrossEntropyLoss(weight=class_weights)
    loss=loss_fn(logits,labels)
    return (loss,outputs) if return_outputs else loss

from transformers import AutoModelForSequenceClassification
model=AutoModelForSequenceClassification.from_pretrained(model_ckpt,
                                                         num_labels=6,
                                                         id2label=id2label,
                                                         label2id=label2id)

from sklearn.metrics import f1_score
def compute_metrics(pred):
  labels=pred.label_ids
  preds=pred.predictions.argmax(-1)
  f1=f1_score(labels,preds,average="weighted")
  return {"f1":f1}

from transformers import TrainingArguments
batch_size=64
logging_steps=len(emotion_dataset["train"])
output_dir="minilm-finetuned-emotion"
training_args=TrainingArguments(output_dir=output_dir,
                                num_train_epochs=5,
                                learning_rate=2e-5,
                                per_device_train_batch_size=batch_size,
                                per_device_eval_batch_size=batch_size,
                                weight_decay=0.01,
                                eval_strategy="epoch",
                                logging_steps=logging_steps,
                                fp16=True,
                                push_to_hub=True)

trainer=weightsLossTrainer(model=model,
                            args=training_args,
                            compute_metrics=compute_metrics,
                            train_dataset=emotion_dataset["train"],
                            eval_dataset=emotion_dataset["validation"],
                            processing_class=tokenizer
                            )

trainer.train()

from transformers import pipeline
model_ckpt="nikk118/minilm-finetuned-emotion"
pipe=pipeline("text-classification",model=model_ckpt)

pipe("i am really excited about gen AI")

